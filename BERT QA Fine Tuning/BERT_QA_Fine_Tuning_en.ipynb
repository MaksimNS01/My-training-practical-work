{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Q&A with BERT-Finetuned"],"metadata":{"id":"fhZy9p42zjDd"}},{"cell_type":"markdown","source":["We will pre-train the BERT model on the [SQuAD1.0](https://rajpurkar.github.io/SQuAD-explorer/) dataset, consisting of questions asked by crowdworkers on a set of Wikipedia articles.\n","\n","Encoder-only models such as BERT are generally excellent at extracting answers to factoid questions like “Who invented the Transformer architecture?”, but poorly at open-ended questions like “Why is the sky blue?”. In such complex cases, encoder-decoder models such as T5 and BART are typically used to synthesize information"],"metadata":{"id":"TPKWDGUVz0B5"}},{"cell_type":"markdown","source":["## Installing libraries"],"metadata":{"id":"sB2EejiG09Dd"}},{"cell_type":"code","source":["!pip install --upgrade datasets fsspec"],"metadata":{"id":"Y_vod-YGUJ7k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install datasets evaluate transformers[sentencepiece]\n","!pip install accelerate\n","# To run the training on TPU, you will need to uncomment the following line:\n","# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n","!apt install git-lfs"],"metadata":{"id":"oWvZMV9hUL4F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git config --global user.email \"your_email\"\n","!git config --global user.name \"your_username\""],"metadata":{"id":"OdMIx85_SEU2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Import libraries and modules"],"metadata":{"id":"tSkh5DBP0cNN"}},{"cell_type":"code","source":["from datasets import load_dataset\n","from huggingface_hub import notebook_login\n","from transformers import AutoTokenizer\n","import torch\n","from transformers import AutoModelForQuestionAnswering\n","import collections\n","import tensorflow as tf\n","from transformers import TFAutoModelForQuestionAnswering\n","import numpy as np\n","import evaluate\n","from tqdm.auto import tqdm\n","from transformers import TrainingArguments\n","from transformers import Trainer\n","from accelerate import Accelerator\n","from torch.utils.data import DataLoader\n","from transformers import default_data_collator\n","from torch.optim import AdamW\n","from transformers import get_scheduler\n","from huggingface_hub import Repository, get_full_repo_name\n","from transformers import pipeline"],"metadata":{"id":"ZGvpQrsjW1W1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Authorization in HuggingFace"],"metadata":{"id":"BNH6cga_6IrI"}},{"cell_type":"code","source":["notebook_login()"],"metadata":{"id":"wIjrXrFj1GoX","executionInfo":{"status":"aborted","timestamp":1751346007358,"user_tz":-420,"elapsed":43,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Loading and analyzing the SQuAD dataset"],"metadata":{"id":"WP959Dx20MFe"}},{"cell_type":"code","source":["raw_datasets = load_dataset(\"squad\")"],"metadata":{"id":"KH7twCx4dJfk","executionInfo":{"status":"aborted","timestamp":1751346007360,"user_tz":-420,"elapsed":44,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QdnTJbHwyNVb","executionInfo":{"status":"aborted","timestamp":1751346007361,"user_tz":-420,"elapsed":45,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"outputs":[],"source":["raw_datasets"]},{"cell_type":"markdown","source":["It looks like we have everything we need in the context, question, and answers fields, so let's output them for the first element of our training set:"],"metadata":{"id":"iLWEh6s30ny5"}},{"cell_type":"code","source":["print(\"Context: \", raw_datasets[\"train\"][0][\"context\"])\n","print(\"Question: \", raw_datasets[\"train\"][0][\"question\"])\n","print(\"Answer: \", raw_datasets[\"train\"][0][\"answers\"])"],"metadata":{"id":"AAJMHmdF0oMF","executionInfo":{"status":"aborted","timestamp":1751346007361,"user_tz":-420,"elapsed":44,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The context and question fields are very easy to use. The answers field is a bit more complicated because it is a dictionary with two fields that are both lists. This is the format that the squad metric will expect when evaluating. The text field is fairly obvious, and the answer_start field contains the index of the starting character of each answer in the context.\n","\n","During training, there is only one possible answer. We can double-check this using the Dataset.filter() method:"],"metadata":{"id":"7UCwId3F1Xhd"}},{"cell_type":"code","source":["raw_datasets[\"train\"].filter(lambda x: len(x[\"answers\"][\"text\"]) != 1)"],"metadata":{"id":"RIp7mmD4dKkP","executionInfo":{"status":"aborted","timestamp":1751346007434,"user_tz":-420,"elapsed":2718,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For evaluation, however, there are several possible answers for each example, which may be the same or different:"],"metadata":{"id":"UrYWhIvK1ht5"}},{"cell_type":"code","source":["print(raw_datasets[\"validation\"][0][\"answers\"])\n","print(raw_datasets[\"validation\"][2][\"answers\"])"],"metadata":{"id":"_t7TMGpB1jNW","executionInfo":{"status":"aborted","timestamp":1751346007435,"user_tz":-420,"elapsed":2717,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" The idea is that some questions have multiple possible answers, and this script will compare the predicted answer with all the valid answers and choose the best result. If we look, for example, at a sample with an index of 2:"],"metadata":{"id":"gLW1OnQN1obV"}},{"cell_type":"code","source":["print(raw_datasets[\"validation\"][2][\"context\"])\n","print(raw_datasets[\"validation\"][2][\"question\"])"],"metadata":{"id":"DLuSmciw1ptp","executionInfo":{"status":"aborted","timestamp":1751346007436,"user_tz":-420,"elapsed":2716,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The answer may indeed be one of the three possible options we saw earlier."],"metadata":{"id":"boQ_BjUD1tAl"}},{"cell_type":"markdown","source":["## Preparing training data"],"metadata":{"id":"nDSS98zb1u5y"}},{"cell_type":"markdown","source":["First, we need to convert the text in the input data into identifiers that the model can understand using the tokenizer:"],"metadata":{"id":"iu_NZliy12gx"}},{"cell_type":"code","source":["model_checkpoint = \"bert-base-cased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"],"metadata":{"id":"mqfm_CmVdLmN","executionInfo":{"status":"aborted","timestamp":1751346007437,"user_tz":-420,"elapsed":2716,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To check that the tokenizer object being used is actually supported, its is_fast attribute is used:"],"metadata":{"id":"xNjkfEaD1_gV"}},{"cell_type":"code","source":["tokenizer.is_fast"],"metadata":{"id":"XzaasCOL1_Ep","executionInfo":{"status":"aborted","timestamp":1751346007437,"user_tz":-420,"elapsed":2714,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can pass our tokenizer the question and context together, and it will correctly insert special tokens to form a sentence like this one:"],"metadata":{"id":"cD6dT6SK2G8a"}},{"cell_type":"code","source":["context = raw_datasets[\"train\"][0][\"context\"]\n","question = raw_datasets[\"train\"][0][\"question\"]\n","\n","inputs = tokenizer(question, context)\n","tokenizer.decode(inputs[\"input_ids\"])"],"metadata":{"id":"ORw-ragU2GZ1","executionInfo":{"status":"aborted","timestamp":1751346007438,"user_tz":-420,"elapsed":2713,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In this case, the context is not too long, but some examples in the dataset have very long contexts that will exceed the maximum length we set (which in this case is 384).\n","\n","To see how this works for the current example, we can limit the length to 100 and use a sliding window of 50 tokens. We use:\n","* max_length (we'll take 100)\n","* truncation=\"only_second\" to truncate the context (which is in the second position) when a question with its context is too long\n","* stride to set the number of overlapping tokens between two consecutive fragments (let's take 50)\n","* return_overflowing_tokens=True to tell the tokenizer that we need overflowing tokens (overflowing tokens)"],"metadata":{"id":"Hauze8Ae2aNF"}},{"cell_type":"code","source":["inputs = tokenizer(\n","    question,\n","    context,\n","    max_length=100,\n","    truncation=\"only_second\",\n","    stride=50,\n","    return_overflowing_tokens=True,\n",")\n","\n","for ids in inputs[\"input_ids\"]:\n","    print(tokenizer.decode(ids))"],"metadata":{"id":"6PDOPD762May","executionInfo":{"status":"aborted","timestamp":1751346007439,"user_tz":-420,"elapsed":2712,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As we can see, our example has been split into four inputs, each containing a question and a piece of context. The answer to the question (“Bernadette Soubirous”) only appears in the third and last input, so by working with long contexts in this way, we will create some training examples in which the answer is not included in the context. For these examples, the labels will be start_position = end_position = 0 (this is how we predict the [CLS] token). We will also set these labels in the unfortunate case where the response has been truncated so that we only have its start (or end). For examples where the response is entirely in context, the labels will be the index of the token where the response begins and the index of the token where the response ends.\n","\n","The dataset gives us the starting character of the answer in context, and by adding the length of the answer to it, we can find the ending character in context. To map these to token indices, we need to use offset mapping6. We can configure our tokenizer to return them by passing return_offsets_mapping=True:"],"metadata":{"id":"7aB1yEMp2qut"}},{"cell_type":"code","source":["inputs = tokenizer(\n","    question,\n","    context,\n","    max_length=100,\n","    truncation=\"only_second\",\n","    stride=50,\n","    return_overflowing_tokens=True,\n","    return_offsets_mapping=True,\n",")\n","inputs.keys()"],"metadata":{"id":"F4trBIN623uK","executionInfo":{"status":"aborted","timestamp":1751346007442,"user_tz":-420,"elapsed":2712,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As we can see, we get back the usual input identifiers, token type identifiers and attention mask, as well as the offset mapping we need and an additional overflow_to_sample_mapping key. We will use the corresponding value when tokenizing multiple texts at the same time (which we must do to take advantage of the fact that our tokenizer is Rust-based). Since a single pattern can produce multiple features, it maps each feature to the example from which it originated. Since we have only tokenized one example here, we will get a list of 0:"],"metadata":{"id":"0itGTQOk26gZ"}},{"cell_type":"code","source":["inputs[\"overflow_to_sample_mapping\"]"],"metadata":{"id":"BfcL1pl12--R","executionInfo":{"status":"aborted","timestamp":1751346007443,"user_tz":-420,"elapsed":2711,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["But if we tokenize more examples, it will become more efficient:"],"metadata":{"id":"rPjVf94O3Ats"}},{"cell_type":"code","source":["inputs = tokenizer(\n","    raw_datasets[\"train\"][2:6][\"question\"],\n","    raw_datasets[\"train\"][2:6][\"context\"],\n","    max_length=100,\n","    truncation=\"only_second\",\n","    stride=50,\n","    return_overflowing_tokens=True,\n","    return_offsets_mapping=True,\n",")\n","\n","print(f\"The 4 examples gave {len(inputs['input_ids'])} features.\")\n","print(f\"Here is where each comes from: {inputs['overflow_to_sample_mapping']}.\")"],"metadata":{"id":"PqEN52yA26Jd","executionInfo":{"status":"aborted","timestamp":1751346007444,"user_tz":-420,"elapsed":2710,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As we can see, the first three examples (with indices 2, 3, and 4 in the training set) produced four features each, and the last example (with index 5 in the training set) produced 7 features."],"metadata":{"id":"YPQBZABU3GzN"}},{"cell_type":"markdown","source":["This information will be useful for matching each obtained feature with the corresponding label. These labels are:\n","\n","* (0, 0) if the answer is not in the corresponding context area\n","* (start_position, end_position) if the answer is in the corresponding context area, with start_position being the token index (in input identifiers) at the beginning of the answer and end_position being the token index (in input identifiers) at the end of the answer."],"metadata":{"id":"_QWslWhH3JDu"}},{"cell_type":"markdown","source":["To determine which of these cases occurs and, if necessary, the positions of the tokens, we first find the indices where the context begins and ends in the input identifiers. We could use token type identifiers for this, but since they don't necessarily exist for all models (for example, DistilBERT doesn't require them), we'll instead use the sequence_ids() method from BatchEncoding, which returns our tokenizer."],"metadata":{"id":"TUWMzcwN3NRK"}},{"cell_type":"markdown","source":["Once we have the token indices, we look at the corresponding offsets, which are tuples of two integers denoting the character spacing within the original context. In this way, we can determine whether the context fragment in this token begins after the response or ends before the response begins (in which case the label would be (0, 0)). If it does not, we loop to find the first and last token of the response:"],"metadata":{"id":"dqGY9NlK3Q8t"}},{"cell_type":"code","source":["answers = raw_datasets[\"train\"][2:6][\"answers\"]\n","start_positions = []\n","end_positions = []\n","\n","for i, offset in enumerate(inputs[\"offset_mapping\"]):\n","    sample_idx = inputs[\"overflow_to_sample_mapping\"][i]\n","    answer = answers[sample_idx]\n","    start_char = answer[\"answer_start\"][0]\n","    end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n","    sequence_ids = inputs.sequence_ids(i)\n","\n","    # Let's find the beginning and end of the context\n","    idx = 0\n","    while sequence_ids[idx] != 1:\n","        idx += 1\n","    context_start = idx\n","    while sequence_ids[idx] == 1:\n","        idx += 1\n","    context_end = idx - 1\n","\n","    # If the response is not completely within context, the label will be (0, 0)\n","    if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n","        start_positions.append(0)\n","        end_positions.append(0)\n","    else:\n","        # Otherwise, these are the start and end positions of the tokens\n","        idx = context_start\n","        while idx <= context_end and offset[idx][0] <= start_char:\n","            idx += 1\n","        start_positions.append(idx - 1)\n","\n","        idx = context_end\n","        while idx >= context_start and offset[idx][1] >= end_char:\n","            idx -= 1\n","        end_positions.append(idx + 1)\n","\n","start_positions, end_positions"],"metadata":{"id":"GHBSBIVU3UYl","executionInfo":{"status":"aborted","timestamp":1751346007444,"user_tz":-420,"elapsed":2708,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's look at a few results to see if our approach is correct. For the first feature we find (83, 85) as labels, so let's compare the theoretical answer with the decoded range of tokens 83 through 85 (inclusive):"],"metadata":{"id":"KtGR5aD13kEV"}},{"cell_type":"code","source":["idx = 0\n","sample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\n","answer = answers[sample_idx][\"text\"][0]\n","\n","start = start_positions[idx]\n","end = end_positions[idx]\n","labeled_answer = tokenizer.decode(inputs[\"input_ids\"][idx][start : end + 1])\n","\n","print(f\"Theoretical answer: {answer}, labels give: {labeled_answer}\")"],"metadata":{"id":"Lr_qajx23kap","executionInfo":{"status":"aborted","timestamp":1751346007445,"user_tz":-420,"elapsed":2707,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["So that's a match! Now let's check index 4, where we set the labels to (0, 0), which means that the answer is not in the context fragment of this feature:"],"metadata":{"id":"ki-BUB9e3n-6"}},{"cell_type":"code","source":["idx = 4\n","sample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\n","answer = answers[sample_idx][\"text\"][0]\n","\n","decoded_example = tokenizer.decode(inputs[\"input_ids\"][idx])\n","print(f\"Theoretical answer: {answer}, decoded example: {decoded_example}\")"],"metadata":{"id":"m0qNuLw_3oMg","executionInfo":{"status":"aborted","timestamp":1751346007506,"user_tz":-420,"elapsed":2767,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["There is no answer in context."],"metadata":{"id":"FLivZQYS3qjG"}},{"cell_type":"markdown","source":["Now that we've dealt with the preprocessing of the training data step by step, we can group it into a feature that we'll apply to the entire dataset. We will augment each feature to the maximum length we have set, as most contexts will be long (and the corresponding samples will be split into multiple features), so applying dynamic augmentation is of no real use here:"],"metadata":{"id":"4dNk5KGa3s0J"}},{"cell_type":"code","source":["max_length = 384\n","stride = 128\n","\n","\n","def preprocess_training_examples(examples):\n","    questions = [q.strip() for q in examples[\"question\"]]\n","    inputs = tokenizer(\n","        questions,\n","        examples[\"context\"],\n","        max_length=max_length,\n","        truncation=\"only_second\",\n","        stride=stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    offset_mapping = inputs.pop(\"offset_mapping\")\n","    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n","    answers = examples[\"answers\"]\n","    start_positions = []\n","    end_positions = []\n","\n","    for i, offset in enumerate(offset_mapping):\n","        sample_idx = sample_map[i]\n","        answer = answers[sample_idx]\n","        start_char = answer[\"answer_start\"][0]\n","        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n","        sequence_ids = inputs.sequence_ids(i)\n","\n","        # Let's find the beginning and end of the context\n","        idx = 0\n","        while sequence_ids[idx] != 1:\n","            idx += 1\n","        context_start = idx\n","        while sequence_ids[idx] == 1:\n","            idx += 1\n","        context_end = idx - 1\n","\n","        # If the response is not completely within context, the label will be (0, 0)\n","        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n","            start_positions.append(0)\n","            end_positions.append(0)\n","        else:\n","            # Otherwise, these are the start and end positions of the tokens\n","            idx = context_start\n","            while idx <= context_end and offset[idx][0] <= start_char:\n","                idx += 1\n","            start_positions.append(idx - 1)\n","\n","            idx = context_end\n","            while idx >= context_start and offset[idx][1] >= end_char:\n","                idx -= 1\n","            end_positions.append(idx + 1)\n","\n","    inputs[\"start_positions\"] = start_positions\n","    inputs[\"end_positions\"] = end_positions\n","    return inputs"],"metadata":{"id":"P36Vdot83zTt","executionInfo":{"status":"aborted","timestamp":1751346007508,"user_tz":-420,"elapsed":2768,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note that we defined two constants to define the maximum length and the sliding window length, and added some cleanup before tokenization: some questions in the SQuAD dataset have extra spaces at the beginning and end that don't add anything, so we removed those extra spaces."],"metadata":{"id":"APkw0apl37Xi"}},{"cell_type":"markdown","source":["To apply this feature to the entire training set, we use the Dataset.map() method with the batched=True flag. This is necessary because we change the length of the dataset (since one example can produce multiple training features):"],"metadata":{"id":"EP8O0_Ne4BNr"}},{"cell_type":"code","source":["train_dataset = raw_datasets[\"train\"].map(\n","    preprocess_training_examples,\n","    batched=True,\n","    remove_columns=raw_datasets[\"train\"].column_names,\n",")\n","len(raw_datasets[\"train\"]), len(train_dataset)"],"metadata":{"id":"r4BGC3NCdPfG","executionInfo":{"status":"aborted","timestamp":1751346007509,"user_tz":-420,"elapsed":2769,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As we can see, the preprocessing has added about 1,000 features. Now our training set is ready for use. Let's move on to preprocessing of the validation set."],"metadata":{"id":"l9hjEAeR4NUZ"}},{"cell_type":"markdown","source":["## Preparing validation data"],"metadata":{"id":"Si21dj_U4MeJ"}},{"cell_type":"markdown","source":["Pre-processing validation data will be a bit easier since we don't need to generate labels. The real joy will be interpreting the model predictions in the ranges of the original context. To do this, we need to store both offset mappings and a way to map each generated feature to the original example from which it is taken. Since the original dataset has an ID column, we will use that ID."],"metadata":{"id":"Tgt5tHXT4UAt"}},{"cell_type":"markdown","source":["We need to clean up the offset mappings. They will contain offsets for the question and context, but at post-processing stage we will have no way to know which part of the input identifiers corresponds to the context and which to the question (the sequence_ids() method we used is only available for tokenizer output). Therefore, we will set the offsets corresponding to the question to None:"],"metadata":{"id":"T3NBhOeV4bOm"}},{"cell_type":"code","source":["def preprocess_validation_examples(examples):\n","    questions = [q.strip() for q in examples[\"question\"]]\n","    inputs = tokenizer(\n","        questions,\n","        examples[\"context\"],\n","        max_length=max_length,\n","        truncation=\"only_second\",\n","        stride=stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n","    example_ids = []\n","\n","    for i in range(len(inputs[\"input_ids\"])):\n","        sample_idx = sample_map[i]\n","        example_ids.append(examples[\"id\"][sample_idx])\n","\n","        sequence_ids = inputs.sequence_ids(i)\n","        offset = inputs[\"offset_mapping\"][i]\n","        inputs[\"offset_mapping\"][i] = [\n","            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n","        ]\n","\n","    inputs[\"example_id\"] = example_ids\n","    return inputs"],"metadata":{"id":"2QK64rl94ioi","executionInfo":{"status":"aborted","timestamp":1751346007510,"user_tz":-420,"elapsed":2770,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["validation_dataset = raw_datasets[\"validation\"].map(\n","    preprocess_validation_examples,\n","    batched=True,\n","    remove_columns=raw_datasets[\"validation\"].column_names,\n",")\n","len(raw_datasets[\"validation\"]), len(validation_dataset)"],"metadata":{"id":"CbXgdYIadShu","executionInfo":{"status":"aborted","timestamp":1751346007511,"user_tz":-420,"elapsed":2771,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In this case, we only added a couple hundred examples, so the contexts in the validation dataset are a bit shorter.\n","\n","Now that we have pre-processed all the data, we can start training."],"metadata":{"id":"IaGoFTvT4p3-"}},{"cell_type":"markdown","source":["## Fine-Tuning"],"metadata":{"id":"j07nDkmK4sm6"}},{"cell_type":"markdown","source":["### Post-processing"],"metadata":{"id":"JSe8RxYW5Rfe"}},{"cell_type":"markdown","source":["The model will output the logits for the start and end positions of the response in the input identifiers.\n","\n","To speed up the process, we will also not estimate all possible pairs (start_token, end_token), but only those corresponding to the largest n_best logits (with n_best=20). Since we will skip softmax, these estimates will be logit estimates, and will be obtained by taking the sum of the start and end logits (instead of the product, by the rule \\(\\log(ab) = \\log(a) + \\log(b)\\)).\n","\n","To demonstrate all of this, we will need some predictions. Since we haven't trained our model yet, we will use the default model for the QA pipeline to generate some predictions on a small portion of the validation set. We can use the same processing function as before; since it relies on the global constant tokenizer, we just need to change that object to the tokenizer of the model we want to use temporarily:"],"metadata":{"id":"k4G-XhEj5i2W"}},{"cell_type":"code","source":["small_eval_set = raw_datasets[\"validation\"].select(range(100))\n","trained_checkpoint = \"distilbert-base-cased-distilled-squad\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n","eval_set = small_eval_set.map(\n","    preprocess_validation_examples,\n","    batched=True,\n","    remove_columns=raw_datasets[\"validation\"].column_names,\n",")"],"metadata":{"id":"j1DGevmjdTll","executionInfo":{"status":"aborted","timestamp":1751346007512,"user_tz":-420,"elapsed":2772,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that preprocessing is complete, we change the tokenizer back to the one originally selected:"],"metadata":{"id":"QJQW04Sw5wyB"}},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"],"metadata":{"id":"2h_P5XHF5yLB","executionInfo":{"status":"aborted","timestamp":1751346007512,"user_tz":-420,"elapsed":2772,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then we remove the columns from our eval_set that the model doesn't expect, create a batch with all that little validation, and run it through the model."],"metadata":{"id":"DM9yEn945zmO"}},{"cell_type":"code","source":["eval_set_for_model = eval_set.remove_columns([\"example_id\", \"offset_mapping\"])\n","eval_set_for_model.set_format(\"torch\")\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","batch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names}\n","trained_model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint).to(\n","    device\n",")\n","\n","with torch.no_grad():\n","    outputs = trained_model(**batch)"],"metadata":{"id":"Bodc7Hq0dUb8","executionInfo":{"status":"aborted","timestamp":1751346007520,"user_tz":-420,"elapsed":2779,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Since Trainer will return our predictions to us as NumPy arrays, we take the start and end logits and convert them to this format:"],"metadata":{"id":"0nybj5dx6Am6"}},{"cell_type":"code","source":["start_logits = outputs.start_logits.cpu().numpy()\n","end_logits = outputs.end_logits.cpu().numpy()"],"metadata":{"id":"q3i3zidA6BFl","executionInfo":{"status":"aborted","timestamp":1751346007521,"user_tz":-420,"elapsed":2780,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we need to find the predicted answer for each example in small_eval_set. One example can be split into multiple features in eval_set, so the first step is to map each example in small_eval_set to the corresponding features in eval_set:"],"metadata":{"id":"zOrGf_HC6Qjt"}},{"cell_type":"code","source":["example_to_features = collections.defaultdict(list)\n","for idx, feature in enumerate(eval_set):\n","    example_to_features[feature[\"example_id\"]].append(idx)"],"metadata":{"id":"mPY_asgN6SdR","executionInfo":{"status":"aborted","timestamp":1751346007522,"user_tz":-420,"elapsed":2781,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["With this in hand, we can proceed by iterating over all examples and, for each example, all associated features. As we said, we consider logit estimates for n_best initial logits and final logits, excluding items that give:\n","\n","An answer that does not fit into the context\n","An answer with negative length\n","An answer that is too long (we limit the possibilities by max_answer_length=30).\n","Once we have all possible answers for a single example, we simply select the one with the best logit score:"],"metadata":{"id":"FCmkNCP37lI-"}},{"cell_type":"code","source":["n_best = 20\n","max_answer_length = 30\n","predicted_answers = []\n","\n","for example in small_eval_set:\n","    example_id = example[\"id\"]\n","    context = example[\"context\"]\n","    answers = []\n","\n","    for feature_index in example_to_features[example_id]:\n","        start_logit = start_logits[feature_index]\n","        end_logit = end_logits[feature_index]\n","        offsets = eval_set[\"offset_mapping\"][feature_index]\n","\n","        start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n","        end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n","        for start_index in start_indexes:\n","            for end_index in end_indexes:\n","                # Skip answers that do not fully match the context\n","                if offsets[start_index] is None or offsets[end_index] is None:\n","                    continue\n","                # Skip responses that are either < 0 or > max_answer_length\n","                if (\n","                    end_index < start_index\n","                    or end_index - start_index + 1 > max_answer_length\n","                ):\n","                    continue\n","\n","                answers.append(\n","                    {\n","                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n","                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n","                    }\n","                )\n","\n","    best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n","    predicted_answers.append({\"id\": example_id, \"prediction_text\": best_answer[\"text\"]})"],"metadata":{"id":"Y4hhUWkl7l-i","executionInfo":{"status":"aborted","timestamp":1751346007522,"user_tz":-420,"elapsed":2781,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The final format of the predicted responses is the one expected by the metric we will use. As usual, we can load it using the Evaluate library:"],"metadata":{"id":"hKk_RN9m77Ru"}},{"cell_type":"code","source":["metric = evaluate.load(\"squad\")"],"metadata":{"id":"sACCwrgidVhG","executionInfo":{"status":"aborted","timestamp":1751346007530,"user_tz":-420,"elapsed":2789,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This metric expects predicted answers in the format we saw above (a list of dictionaries with one key for the example ID and one key for the predicted text) and theoretical answers in the format below (a list of dictionaries with one key for the example ID and one key for possible answers):"],"metadata":{"id":"8t2cgtu07-r-"}},{"cell_type":"code","source":["theoretical_answers = [\n","    {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in small_eval_set\n","]"],"metadata":{"id":"AZMSmRme7--q","executionInfo":{"status":"aborted","timestamp":1751346007531,"user_tz":-420,"elapsed":2790,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can now verify that we are getting reasonable results by looking at the first element of both lists:"],"metadata":{"id":"uTGubktr8BdU"}},{"cell_type":"code","source":["print(predicted_answers[0])\n","print(theoretical_answers[0])"],"metadata":{"id":"yTmxZv8d8CR2","executionInfo":{"status":"aborted","timestamp":1751346007542,"user_tz":-420,"elapsed":2799,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["That's not too bad! Now let's look at the score that the metric gives us:"],"metadata":{"id":"ML8hs8tW8Dw2"}},{"cell_type":"code","source":["metric.compute(predictions=predicted_answers, references=theoretical_answers)"],"metadata":{"id":"u8YxIbNS8EL2","executionInfo":{"status":"aborted","timestamp":1751346007543,"user_tz":-420,"elapsed":2798,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now let's put everything we just did into the compute_metrics() function that we will use in Trainer. Normally, compute_metrics() only gets the eval_preds tuple with logits and labels. Here we'll need a bit more than that, since we have to look in the bias trait dataset and in the example dataset for the original contexts, so we won't be able to use this function to get the usual eval results during training. We will only use it at the end of training to check the results.\n","\n","The compute_metrics() function groups the same steps as before, only a small check is added in case we do not find any correct answers (in which case we predict an empty string)."],"metadata":{"id":"ig2aYgac8Rna"}},{"cell_type":"code","source":["def compute_metrics(start_logits, end_logits, features, examples):\n","    example_to_features = collections.defaultdict(list)\n","    for idx, feature in enumerate(features):\n","        example_to_features[feature[\"example_id\"]].append(idx)\n","\n","    predicted_answers = []\n","    for example in tqdm(examples):\n","        example_id = example[\"id\"]\n","        context = example[\"context\"]\n","        answers = []\n","\n","        # Iterate over all the features associated with this example\n","        for feature_index in example_to_features[example_id]:\n","            start_logit = start_logits[feature_index]\n","            end_logit = end_logits[feature_index]\n","            offsets = features[feature_index][\"offset_mapping\"]\n","\n","            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n","            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n","            for start_index in start_indexes:\n","                for end_index in end_indexes:\n","                    # Skip answers that don't fully fit the context\n","                    if offsets[start_index] is None or offsets[end_index] is None:\n","                        continue\n","                    # Skip responses whose length is either < 0 or > max_answer_length\n","                    if (\n","                        end_index < start_index\n","                        or end_index - start_index + 1 > max_answer_length\n","                    ):\n","                        continue\n","\n","                    answer = {\n","                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n","                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n","                    }\n","                    answers.append(answer)\n","\n","        # Choose the answer with the best score\n","        if len(answers) > 0:\n","            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n","            predicted_answers.append(\n","                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n","            )\n","        else:\n","            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n","\n","    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n","    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"],"metadata":{"id":"Mp9IeV9p8W62","executionInfo":{"status":"aborted","timestamp":1751346007544,"user_tz":-420,"elapsed":2799,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["compute_metrics(start_logits, end_logits, eval_set, small_eval_set)"],"metadata":{"id":"046aTymsdXKs","executionInfo":{"status":"aborted","timestamp":1751346007544,"user_tz":-420,"elapsed":2799,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Looks great! Now let's use this to pre-train our model."],"metadata":{"id":"WXalrjNm8gZN"}},{"cell_type":"markdown","source":["### Model retraining"],"metadata":{"id":"i7A557My8hNW"}},{"cell_type":"markdown","source":["Now we are ready to train our model. Let's first create it using the AutoModelForQuestionAnswering class as before:"],"metadata":{"id":"npFiBjJ58l66"}},{"cell_type":"code","source":["model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"],"metadata":{"id":"QYmJr-fhdYOS","executionInfo":{"status":"aborted","timestamp":1751346007545,"user_tz":-420,"elapsed":2800,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As usual, we get a warning that some weights are not used (pre-training head weights) and others are initialized randomly (question answering head weights). You should be used to this by now, but it means that the model is not yet ready to be used and needs to be pre-trained - good thing we are about to do that!"],"metadata":{"id":"SnGY-c2J8tJG"}},{"cell_type":"markdown","source":["We can define our TrainingArguments. As we said when we defined our function to compute a metric, we won't be able to do a normal evaluation loop because of the signature of compute_metrics(). We could write our own Trainer subclass for this, but that's too long for this section. Instead, we will only evaluate the model at the end of training, and we will show how to do regular evaluation below in the section “Custom Training Loop”."],"metadata":{"id":"jjLz31tI8yfK"}},{"cell_type":"code","source":["args = TrainingArguments(\n","    \"bert-finetuned-squad\",\n","    # evaluation_strategy=\"no\",\n","    save_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n","    fp16=True,\n","    push_to_hub=True,\n",")"],"metadata":{"id":"w5qbAL-r8tfC","executionInfo":{"status":"aborted","timestamp":1751346007546,"user_tz":-420,"elapsed":2801,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, we just pass everything to the Trainer class and run the training:"],"metadata":{"id":"vguKWDBp9s6B"}},{"cell_type":"code","source":["trainer = Trainer(\n","    model=model,\n","    args=args,\n","    train_dataset=train_dataset,\n","    eval_dataset=validation_dataset,\n","    tokenizer=tokenizer,\n",")\n","trainer.train()"],"metadata":{"id":"TBSU0ytae63Q","executionInfo":{"status":"aborted","timestamp":1751346007547,"user_tz":-420,"elapsed":2802,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["When training is complete, we can finally evaluate our model (and pray we didn't waste all that computation time). The predict() method of the Trainer function will return a tuple where the first elements are the model predictions (here a pair with the initial and final logits). We send it to our compute_metrics() function:"],"metadata":{"id":"5u_2AwYJ-Lcr"}},{"cell_type":"code","source":["predictions, _, _ = trainer.predict(validation_dataset)\n","start_logits, end_logits = predictions\n","compute_metrics(start_logits, end_logits, validation_dataset, raw_datasets[\"validation\"])"],"metadata":{"id":"HGvXOlhTdZ0B","executionInfo":{"status":"aborted","timestamp":1751346007548,"user_tz":-420,"elapsed":2802,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Great!!! For comparison, the baseline numbers listed in the BERT article for this model are 80.8 and 88.5, so we're right where we should be."],"metadata":{"id":"K2OgZAT1-S-7"}},{"cell_type":"markdown","source":["Finally, we use the push_to_hub() method to make sure we have loaded the latest version of the model:"],"metadata":{"id":"8siqvk_C-UG6"}},{"cell_type":"code","source":["trainer.push_to_hub(commit_message=\"Training complete\")"],"metadata":{"id":"RXlm5idTUSM9","executionInfo":{"status":"aborted","timestamp":1751346007548,"user_tz":-420,"elapsed":2802,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This returns the URL of the commit that was just executed."],"metadata":{"id":"Ovigua_w-XxS"}},{"cell_type":"markdown","source":["Trainer also creates a draft model card with all of the evaluation results and uploads it."],"metadata":{"id":"_k7OvJlU-Ze8"}},{"cell_type":"markdown","source":["### Training Cycle"],"metadata":{"id":"Zuf6ROL6-cCV"}},{"cell_type":"markdown","source":["First we need to create DataLoaders from our datasets. We will set the format of these datasets to “torch” and remove columns in the validation set that are not used by the model. Then we can use the default_data_collator provided by Transformers as collate_fn and mix the training set but not the validation set:"],"metadata":{"id":"flg_2BPo-kpZ"}},{"cell_type":"code","source":["train_dataset.set_format(\"torch\")\n","validation_set = validation_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n","validation_set.set_format(\"torch\")\n","\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    shuffle=True,\n","    collate_fn=default_data_collator,\n","    batch_size=8,\n",")\n","eval_dataloader = DataLoader(\n","    validation_set, collate_fn=default_data_collator, batch_size=8\n",")"],"metadata":{"id":"rRdVPiuY-lLV","executionInfo":{"status":"aborted","timestamp":1751346007596,"user_tz":-420,"elapsed":2850,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We then reinstantiate our model to ensure that we do not continue pre-training, but start again with the pre-trained BERT model:"],"metadata":{"id":"6wBMiHfK-oQ2"}},{"cell_type":"code","source":["model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"],"metadata":{"id":"GGU5Uptd-ovx","executionInfo":{"status":"aborted","timestamp":1751346007598,"user_tz":-420,"elapsed":2851,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then we need an optimizer. As usual, we use the classic AdamW, which is similar to Adam, but with a fix in the way weight attenuation is applied:"],"metadata":{"id":"wcK6m_sA-q6p"}},{"cell_type":"code","source":["optimizer = AdamW(model.parameters(), lr=2e-5)"],"metadata":{"id":"V4rzbotQ-rVC","executionInfo":{"status":"aborted","timestamp":1751346007600,"user_tz":-420,"elapsed":2852,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# accelerator = Accelerator(fp16=True)\n","accelerator = Accelerator(mixed_precision=\"fp16\")\n","model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n","    model, optimizer, train_dataloader, eval_dataloader\n",")"],"metadata":{"id":"kusRHiDD--Fz","executionInfo":{"status":"aborted","timestamp":1751346007601,"user_tz":-420,"elapsed":2853,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can use the length of train_dataloader to compute the number of training steps only after it passes through the accelerator.prepare() method. We use the same line graph as in the previous sections:"],"metadata":{"id":"FOeWxKKGBGRm"}},{"cell_type":"code","source":["num_train_epochs = 2\n","num_update_steps_per_epoch = len(train_dataloader)\n","num_training_steps = num_train_epochs * num_update_steps_per_epoch\n","\n","lr_scheduler = get_scheduler(\n","    \"linear\",\n","    optimizer=optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=num_training_steps,\n",")"],"metadata":{"id":"gxSXgJfWBHln","executionInfo":{"status":"aborted","timestamp":1751346007602,"user_tz":-420,"elapsed":2854,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To send our model to the Hub, we need to create a Repository object in the working folder. First, log into the Hugging Face Hub if you are not already logged in. We'll define the name of the repository by the model ID we want to assign to our model (feel free to replace repo_name with your own; it just needs to contain your username, which is what get_full_repo_name() does):"],"metadata":{"id":"ryPiO5HTBKQw"}},{"cell_type":"code","source":["model_name = \"bert-finetuned-squad-accelerate\"\n","repo_name = get_full_repo_name(model_name)\n","repo_name"],"metadata":{"id":"u8xqp8G_BKuN","executionInfo":{"status":"aborted","timestamp":1751346007602,"user_tz":-420,"elapsed":2852,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The code below will cause a “Repo not found” error. The HuggingFace community has found the following solution to the problem:\n","* Go to the last line of the error and find which repository is not found.\n","* Go to [Hugging Face - The AI community building the future](https://huggingface.co/new) to create the repository that is missing.\n","* Run the code again"],"metadata":{"id":"Pnvb-XDYgtKi"}},{"cell_type":"code","source":["output_dir = \"bert-finetuned-squad-accelerate\"\n","repo = Repository(output_dir, clone_from=repo_name)"],"metadata":{"id":"o5kObeGCUT0E","executionInfo":{"status":"aborted","timestamp":1751346007603,"user_tz":-420,"elapsed":2853,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training cycle"],"metadata":{"id":"VJUnE0EXBO9h"}},{"cell_type":"markdown","source":["We are now ready to write a complete training cycle. After defining a progress bar to keep track of the training progress, the loop consists of three parts:\n","\n","Training proper, which is a classical iteration on train_dataloader, a forward pass through the model, then a backward pass and an optimizer step.\n","Evaluation, in which we collect all the values for start_logits and end_logits before converting them into NumPy arrays. Once the evaluation loop is complete, we combine all the results. Note that we need to do truncation because Accelerator may add some examples at the end to make sure we have the same number of examples in each process.\n","Saving and loading, where we first save the model and tokenizer, and then call repo.push_to_hub(). As before, we use the blocking=False argument to tell the 🤗 Hub library to push asynchronously. This way, the learning continues normally and this (long) instruction is executed in the background.\n","Here is the full code for the learning loop:"],"metadata":{"id":"cw3gqk3OBTtq"}},{"cell_type":"code","source":["progress_bar = tqdm(range(num_training_steps))\n","\n","for epoch in range(num_train_epochs):\n","    model.train()\n","    for step, batch in enumerate(train_dataloader):\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        accelerator.backward(loss)\n","\n","        optimizer.step()\n","        lr_scheduler.step()\n","        optimizer.zero_grad()\n","        progress_bar.update(1)\n","\n","    model.eval()\n","    start_logits = []\n","    end_logits = []\n","    accelerator.print(\"Evaluation!\")\n","    for batch in tqdm(eval_dataloader):\n","        with torch.no_grad():\n","            outputs = model(**batch)\n","\n","        start_logits.append(accelerator.gather(outputs.start_logits).cpu().numpy())\n","        end_logits.append(accelerator.gather(outputs.end_logits).cpu().numpy())\n","\n","    start_logits = np.concatenate(start_logits)\n","    end_logits = np.concatenate(end_logits)\n","    start_logits = start_logits[: len(validation_dataset)]\n","    end_logits = end_logits[: len(validation_dataset)]\n","\n","    metrics = compute_metrics(\n","        start_logits, end_logits, validation_dataset, raw_datasets[\"validation\"]\n","    )\n","    print(f\"epoch {epoch}:\", metrics)\n","\n","    accelerator.wait_for_everyone()\n","    unwrapped_model = accelerator.unwrap_model(model)\n","    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n","    if accelerator.is_main_process:\n","        tokenizer.save_pretrained(output_dir)\n","        repo.push_to_hub(\n","            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n","        )"],"metadata":{"id":"bjM9wp1CdblE","executionInfo":{"status":"aborted","timestamp":1751346007604,"user_tz":-420,"elapsed":2854,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Using the Fine-Tuned Model"],"metadata":{"id":"KfmOh-JwBYf9"}},{"cell_type":"markdown","source":["To use it locally in the pipeline, you just need to specify the model ID:"],"metadata":{"id":"s3U0asSnBb9V"}},{"cell_type":"code","source":["model_checkpoint = \"huggingface-course/bert-finetuned-squad\"\n","question_answerer = pipeline(\"question-answering\", model=model_checkpoint)\n","\n","context = \"\"\"\n","Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch and TensorFlow — with a seamless integration\n","between them. It's straightforward to train your models with one before loading them for inference with the other.\n","\"\"\""],"metadata":{"id":"tZt5-NCvhlpZ","executionInfo":{"status":"aborted","timestamp":1751346007604,"user_tz":-420,"elapsed":2851,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# List of questions\n","questions = [\n","    \"Which deep learning libraries back Transformers?\",\n","    \"How many popular deep learning libraries support Transformers?\",\n","    \"Can you switch between libraries after training a model?\",\n","    \"Name a library that integrates with Transformers other than PyTorch.\",\n","    \"What is the main advantage of Transformers' library integration?\"\n","]\n","\n","# Run a model for each question\n","for question in questions:\n","    result = question_answerer(question=question, context=context)\n","    print(f\"Question: {question}\")\n","    print(f\"Answer: {result['answer']} (score: {result['score']:.4f})\\n\")"],"metadata":{"id":"6XqwUr_Qhmwc","executionInfo":{"status":"aborted","timestamp":1751346007605,"user_tz":-420,"elapsed":2850,"user":{"displayName":"Максим Нестеренко","userId":"13588279963652665930"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Great! Our model works just as well as the default model for this pipeline!"],"metadata":{"id":"0gbv8vsnBh4l"}}]}