{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Importing libraries and modules"],"metadata":{"id":"0aaCnDKop7wo"}},{"cell_type":"code","source":["!pip install torch\n","\n","# Installing Hugging Face libraries\n","!pip install transformers datasets accelerate evaluate bitsandbytes trl peft"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HZIKFRC7ldAO","outputId":"a8b57e19-9851-4010-c75b-801b6cee3ad1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.49.0)\n","Collecting datasets\n","  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n","Collecting evaluate\n","  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n","Collecting bitsandbytes\n","  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n","Collecting trl\n","  Downloading trl-0.16.0-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl) (13.9.4)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n","Downloading datasets-3.4.1-py3-none-any.whl (487 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trl-0.16.0-py3-none-any.whl (335 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.7/335.7 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, bitsandbytes, trl, evaluate\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2025.3.0\n","    Uninstalling fsspec-2025.3.0:\n","      Successfully uninstalled fsspec-2025.3.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed bitsandbytes-0.45.3 datasets-3.4.1 dill-0.3.8 evaluate-0.4.3 fsspec-2024.12.0 multiprocess-0.70.16 trl-0.16.0 xxhash-3.5.0\n"]}]},{"cell_type":"code","source":["!pip install llama-cpp-python fire"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0wDYqxEYn5tG","outputId":"d0beee5e-6f89-42e2-ac72-2b1828d5ece1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting llama-cpp-python\n","  Downloading llama_cpp_python-0.3.8.tar.gz (67.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting fire\n","  Downloading fire-0.7.0.tar.gz (87 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.12.2)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (2.0.2)\n","Collecting diskcache>=5.6.1 (from llama-cpp-python)\n","  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire) (2.5.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n","Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: llama-cpp-python, fire\n","  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.8-cp311-cp311-linux_x86_64.whl size=5959658 sha256=3ca0e6f9c22aa28b256c60441acd3440382ab9e3a2248df10ef593734a471403\n","  Stored in directory: /root/.cache/pip/wheels/c0/03/66/eb3810eafd55d921b2be32896d1f44313996982360663aa80b\n","  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=b343349814e27df43ea636bb5835a6e32abe64396309dcefdf91ecb1e37e3d86\n","  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n","Successfully built llama-cpp-python fire\n","Installing collected packages: fire, diskcache, llama-cpp-python\n","Successfully installed diskcache-5.6.3 fire-0.7.0 llama-cpp-python-0.3.8\n"]}]},{"cell_type":"code","source":["!wget https://huggingface.co/IlyaGusev/saiga_mistral_7b_gguf/resolve/main/model-q4_K.gguf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6JlLHUP3nZVG","outputId":"fcad8eb8-f96e-41ed-d6c0-984331f06cbc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-03-25 14:16:51--  https://huggingface.co/IlyaGusev/saiga_mistral_7b_gguf/resolve/main/model-q4_K.gguf\n","Resolving huggingface.co (huggingface.co)... 3.166.152.110, 3.166.152.105, 3.166.152.44, ...\n","Connecting to huggingface.co (huggingface.co)|3.166.152.110|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://cdn-lfs.hf.co/repos/79/b3/79b3fc4694b2c3a22273003a1de570f145c14f0586c212c28c28e302adf5d3d6/2798f33ff63c791a21f05c1ee9a10bc95630b17225c140c197188a3d5cf32644?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-q4_K.gguf%3B+filename%3D%22model-q4_K.gguf%22%3B&Expires=1742915811&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MjkxNTgxMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy83OS9iMy83OWIzZmM0Njk0YjJjM2EyMjI3MzAwM2ExZGU1NzBmMTQ1YzE0ZjA1ODZjMjEyYzI4YzI4ZTMwMmFkZjVkM2Q2LzI3OThmMzNmZjYzYzc5MWEyMWYwNWMxZWU5YTEwYmM5NTYzMGIxNzIyNWMxNDBjMTk3MTg4YTNkNWNmMzI2NDQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=evyOr3S%7EkEtx9RlOedfKifJsQypXaP9XrLcPDNOYcUgpyd3NfV6LRF6Jno2M7lMSPgeSG9j9t4ewWp%7E9bZewmOt1adpy5rAyGVpHfaBaxkkKQ4vl2iqnmZNFA6CFtBjwFryNKBFKhxHoAssGLwEPkeQnXycVYtiGXGH5mp1x2fZaLEf2R%7EmlhAwvZOl-0PHlRH4xqgiZNg3tElqymxWNmfp94Unx9pODJTcEUWtFPZlQ-Fgbs0d9qwNF4ZM1VLWTX%7E1EMmOLFDE9ppRnkWne5Abg0UCz2nR5PtnjttO6X8Upmt9RWGDOJNv-vx6lzXFP3SKTUkmRQFAJg5ny0iaorg__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n","--2025-03-25 14:16:51--  https://cdn-lfs.hf.co/repos/79/b3/79b3fc4694b2c3a22273003a1de570f145c14f0586c212c28c28e302adf5d3d6/2798f33ff63c791a21f05c1ee9a10bc95630b17225c140c197188a3d5cf32644?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-q4_K.gguf%3B+filename%3D%22model-q4_K.gguf%22%3B&Expires=1742915811&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MjkxNTgxMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy83OS9iMy83OWIzZmM0Njk0YjJjM2EyMjI3MzAwM2ExZGU1NzBmMTQ1YzE0ZjA1ODZjMjEyYzI4YzI4ZTMwMmFkZjVkM2Q2LzI3OThmMzNmZjYzYzc5MWEyMWYwNWMxZWU5YTEwYmM5NTYzMGIxNzIyNWMxNDBjMTk3MTg4YTNkNWNmMzI2NDQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=evyOr3S%7EkEtx9RlOedfKifJsQypXaP9XrLcPDNOYcUgpyd3NfV6LRF6Jno2M7lMSPgeSG9j9t4ewWp%7E9bZewmOt1adpy5rAyGVpHfaBaxkkKQ4vl2iqnmZNFA6CFtBjwFryNKBFKhxHoAssGLwEPkeQnXycVYtiGXGH5mp1x2fZaLEf2R%7EmlhAwvZOl-0PHlRH4xqgiZNg3tElqymxWNmfp94Unx9pODJTcEUWtFPZlQ-Fgbs0d9qwNF4ZM1VLWTX%7E1EMmOLFDE9ppRnkWne5Abg0UCz2nR5PtnjttO6X8Upmt9RWGDOJNv-vx6lzXFP3SKTUkmRQFAJg5ny0iaorg__&Key-Pair-Id=K3RPWS32NSSJCE\n","Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 18.173.166.43, 18.173.166.116, 18.173.166.94, ...\n","Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|18.173.166.43|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4368450336 (4.1G) [binary/octet-stream]\n","Saving to: ‘model-q4_K.gguf’\n","\n","model-q4_K.gguf     100%[===================>]   4.07G   235MB/s    in 24s     \n","\n","2025-03-25 14:17:15 (175 MB/s) - ‘model-q4_K.gguf’ saved [4368450336/4368450336]\n","\n"]}]},{"cell_type":"markdown","source":["# Quantized version of the model saiga_mistral_7b_gguf"],"metadata":{"id":"Gb3J-i2pqBWW"}},{"cell_type":"code","source":["%%writefile main.py\n","\n","import fire\n","from llama_cpp import Llama\n","\n","SYSTEM_PROMPT = \"Ты — Сайга, русскоязычный автоматический ассистент. Ты разговариваешь с людьми, помогаешь им и рассказываешь шутки.\"\n","\n","def get_message_tokens(model, role, content):\n","    content = f\"{role}\\n{content}\\n</s>\"\n","    content = content.encode(\"utf-8\")\n","    message_tokens = model.tokenize(content, special=True)\n","    return message_tokens\n","\n","\n","def get_system_tokens(model):\n","    system_message = {\n","        \"role\": \"system\",\n","        \"content\": SYSTEM_PROMPT\n","    }\n","    return get_message_tokens(model, **system_message)\n","\n","\n","def interact(\n","    model_path,\n","    n_ctx=2000,\n","    top_k=30,\n","    top_p=0.9,\n","    temperature=0.2,\n","    repeat_penalty=1.1\n","):\n","    model = Llama(\n","        model_path=model_path,\n","        n_ctx=n_ctx,\n","        n_parts=1,\n","    )\n","\n","    system_tokens = get_system_tokens(model)\n","    tokens = system_tokens\n","    model.eval(tokens)\n","\n","    while True:\n","        user_message = input(\"User: \")\n","        message_tokens = get_message_tokens(model=model, role=\"user\", content=user_message)\n","        role_tokens = model.tokenize(\"bot\\n\".encode(\"utf-8\"), special=True)\n","        tokens += message_tokens + role_tokens\n","        full_prompt = model.detokenize(tokens)\n","        generator = model.generate(\n","            tokens,\n","            top_k=top_k,\n","            top_p=top_p,\n","            temp=temperature,\n","            repeat_penalty=repeat_penalty\n","        )\n","        for token in generator:\n","            token_str = model.detokenize([token]).decode(\"utf-8\", errors=\"ignore\")\n","            tokens.append(token)\n","            if token == model.token_eos():\n","                break\n","            print(token_str, end=\"\", flush=True)\n","        print()\n","\n","\n","if __name__ == \"__main__\":\n","    fire.Fire(interact)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B3tVqmLbni3y","outputId":"4da31b3f-5c01-4042-c96f-ff81d5edf4ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing main.py\n"]}]},{"cell_type":"markdown","source":["# Launch"],"metadata":{"id":"5uu8wjszqH1G"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"u-PvKZ4JkBjd"},"outputs":[],"source":["# inputs = [\n","#            \"Расскажи короткий анекдот про Вовочку и строителей\",\n","#            \"Сочини короткую смешную историю про двух друзей\",\n","#            \"Расскажи короткую шутку про тёщу\"\n","#          ]"]},{"cell_type":"code","source":["# Customize settings for Colab\n","import os\n","os.environ[‘CMAKE_ARGS’] = \"-DLLAMA_CUBLAS=on\"\n","os.environ[‘FORCE_CMAKE’] = \"1\"\n","\n","# Start interactive mode with GPU acceleration\n","!python main.py model-q4_K.gguf \\\n","    --n_ctx 2048 \\\n","    --n-gpu-layers 20 \\\n","    --temp 0.7 \\\n","    --top_k 40 \\\n","    --top_p 0.9"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xbno_8gxoiBo","outputId":"62fdbc10-7d14-4692-96ce-4c44e5a537eb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from model-q4_K.gguf (version GGUF V2)\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = models\n","llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n","llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n","llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n","llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n","llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n","llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n","llama_model_loader: - kv  11:                          general.file_type u32              = 15\n","llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n","llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n","llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n","llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n","llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n","llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   65 tensors\n","llama_model_loader: - type q4_K:  193 tensors\n","llama_model_loader: - type q6_K:   33 tensors\n","print_info: file format = GGUF V2\n","print_info: file type   = Q4_K - Medium\n","print_info: file size   = 4.07 GiB (4.83 BPW) \n","init_tokenizer: initializing tokenizer for type 1\n","load: control-looking token:  32000 '<|im_end|>' was not control-type; this is probably a bug in the model. its type will be overridden\n","load: control token:      2 '</s>' is not marked as EOG\n","load: control token:      1 '<s>' is not marked as EOG\n","load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n","load: special tokens cache size = 5\n","load: token to piece cache size = 0.1637 MB\n","print_info: arch             = llama\n","print_info: vocab_only       = 0\n","print_info: n_ctx_train      = 32768\n","print_info: n_embd           = 4096\n","print_info: n_layer          = 32\n","print_info: n_head           = 32\n","print_info: n_head_kv        = 8\n","print_info: n_rot            = 128\n","print_info: n_swa            = 0\n","print_info: n_embd_head_k    = 128\n","print_info: n_embd_head_v    = 128\n","print_info: n_gqa            = 4\n","print_info: n_embd_k_gqa     = 1024\n","print_info: n_embd_v_gqa     = 1024\n","print_info: f_norm_eps       = 0.0e+00\n","print_info: f_norm_rms_eps   = 1.0e-05\n","print_info: f_clamp_kqv      = 0.0e+00\n","print_info: f_max_alibi_bias = 0.0e+00\n","print_info: f_logit_scale    = 0.0e+00\n","print_info: f_attn_scale     = 0.0e+00\n","print_info: n_ff             = 14336\n","print_info: n_expert         = 0\n","print_info: n_expert_used    = 0\n","print_info: causal attn      = 1\n","print_info: pooling type     = 0\n","print_info: rope type        = 0\n","print_info: rope scaling     = linear\n","print_info: freq_base_train  = 10000.0\n","print_info: freq_scale_train = 1\n","print_info: n_ctx_orig_yarn  = 32768\n","print_info: rope_finetuned   = unknown\n","print_info: ssm_d_conv       = 0\n","print_info: ssm_d_inner      = 0\n","print_info: ssm_d_state      = 0\n","print_info: ssm_dt_rank      = 0\n","print_info: ssm_dt_b_c_rms   = 0\n","print_info: model type       = 7B\n","print_info: model params     = 7.24 B\n","print_info: general.name     = models\n","print_info: vocab type       = SPM\n","print_info: n_vocab          = 32002\n","print_info: n_merges         = 0\n","print_info: BOS token        = 1 '<s>'\n","print_info: EOS token        = 2 '</s>'\n","print_info: EOT token        = 32000 '<|im_end|>'\n","print_info: UNK token        = 0 '<unk>'\n","print_info: PAD token        = 0 '<unk>'\n","print_info: LF token         = 13 '<0x0A>'\n","print_info: EOG token        = 2 '</s>'\n","print_info: EOG token        = 32000 '<|im_end|>'\n","print_info: max token length = 48\n","load_tensors: loading model tensors, this can take a while... (mmap = true)\n","load_tensors: layer   0 assigned to device CPU\n","load_tensors: layer   1 assigned to device CPU\n","load_tensors: layer   2 assigned to device CPU\n","load_tensors: layer   3 assigned to device CPU\n","load_tensors: layer   4 assigned to device CPU\n","load_tensors: layer   5 assigned to device CPU\n","load_tensors: layer   6 assigned to device CPU\n","load_tensors: layer   7 assigned to device CPU\n","load_tensors: layer   8 assigned to device CPU\n","load_tensors: layer   9 assigned to device CPU\n","load_tensors: layer  10 assigned to device CPU\n","load_tensors: layer  11 assigned to device CPU\n","load_tensors: layer  12 assigned to device CPU\n","load_tensors: layer  13 assigned to device CPU\n","load_tensors: layer  14 assigned to device CPU\n","load_tensors: layer  15 assigned to device CPU\n","load_tensors: layer  16 assigned to device CPU\n","load_tensors: layer  17 assigned to device CPU\n","load_tensors: layer  18 assigned to device CPU\n","load_tensors: layer  19 assigned to device CPU\n","load_tensors: layer  20 assigned to device CPU\n","load_tensors: layer  21 assigned to device CPU\n","load_tensors: layer  22 assigned to device CPU\n","load_tensors: layer  23 assigned to device CPU\n","load_tensors: layer  24 assigned to device CPU\n","load_tensors: layer  25 assigned to device CPU\n","load_tensors: layer  26 assigned to device CPU\n","load_tensors: layer  27 assigned to device CPU\n","load_tensors: layer  28 assigned to device CPU\n","load_tensors: layer  29 assigned to device CPU\n","load_tensors: layer  30 assigned to device CPU\n","load_tensors: layer  31 assigned to device CPU\n","load_tensors: layer  32 assigned to device CPU\n","load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n","load_tensors:   CPU_Mapped model buffer size =  4165.38 MiB\n",".................................................................................................\n","llama_init_from_model: n_seq_max     = 1\n","llama_init_from_model: n_ctx         = 2048\n","llama_init_from_model: n_ctx_per_seq = 2048\n","llama_init_from_model: n_batch       = 512\n","llama_init_from_model: n_ubatch      = 512\n","llama_init_from_model: flash_attn    = 0\n","llama_init_from_model: freq_base     = 10000.0\n","llama_init_from_model: freq_scale    = 1\n","llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n","llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n","llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n","llama_init_from_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n","llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n","llama_init_from_model:        CPU compute buffer size =   164.01 MiB\n","llama_init_from_model: graph nodes  = 1030\n","llama_init_from_model: graph splits = 1\n","CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n","Model metadata: {'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'models', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n","Using fallback chat format: llama-2\n","Расскажи короткий анекдот про Вовочку и строителей\n","User: Llama.generate: 58 prefix-match hit, remaining 30 prompt tokens to eval\n","Вовочка, строительный рабочий, был известен своей любовью к алкоголю. Однажды, когда он был в состоянии немного выпить, его начальник попросил его помочь с переносом груза на верхний этаж. Вовочка согласился и начал подниматься по лестнице с грузом.\n","\n","Наверху он остановился, чтобы отдохнуть и выпить немного водки. В это время его начальник пришёл к нему на верхний этаж и сказал: \"Вовочка, ты уже должен был закончить перенос груза\". Вовочка ответил: \"Не так быстро, как ты думаешь, начальник. Я еще не выпил все водку!\"\n","User: Сочини короткую смешную историю про двух друзей\n","Llama.generate: 295 prefix-match hit, remaining 29 prompt tokens to eval\n","Два друга, Борис и Виктор, решили провести вечер в компании. Они заказали пиццу и напитки, а потом устроили игру в карты.\n","\n","В процессе игры они начали спорить о том, кто лучше всего играет. Борис утверждал, что он - чемпион, а Виктор говорил, что он тоже хороший игрок. Они решили разрешить спор с помощью дуэли на пистолетах.\n","\n","Они вышли на улицу и начали стрелять друг в друга. Но тут случайно протаранил их машина, которая была парковка на улице. Оба друга погибли на месте.\n","\n","В итоге, они оказались на небесах, где Борис и Виктор встретили там своего друга-подруга. Они спросили у него, кто из них лучше играет в карты. Тот ответил: \"Вам оба одинаково хороши\".\n","User: Расскажи короткую шутку про тёщу\n","Llama.generate: 600 prefix-match hit, remaining 25 prompt tokens to eval\n","Тёща приехала на праздник. В этот день она решила показать свою новую одежду. Она вышла из ванны, и все увидели, что у неё на груди появился татуированный крест.\n","\n","Тёща сразу же заметила, что все смотрят на неё так интересно, и решила объяснить причину своего нового оформления. Она рассказала: \"Вот почему я себе сделала татуировку - чтобы каждый раз, когда меня заставляют выходить из ванны, я могла напоминать всем о том, что у меня есть душа\".\n","User: Traceback (most recent call last):\n","  File \"/content/main.py\", line 63, in <module>\n","    fire.Fire(interact)\n","  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 135, in Fire\n","    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n","                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 468, in _Fire\n","    component, remaining_args = _CallAndUpdateTrace(\n","                                ^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n","    component = fn(*varargs, **kwargs)\n","                ^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/main.py\", line 41, in interact\n","    user_message = input(\"User: \")\n","                   ^^^^^^^^^^^^^^^\n","KeyboardInterrupt\n","^C\n"]}]},{"cell_type":"markdown","source":["# Estimating the model on the dataset ru_turbo_saiga"],"metadata":{"id":"8QCx6ho7p4wJ"}},{"cell_type":"markdown","source":["Cloning a dataset from Hugging Face"],"metadata":{"id":"DlHuU78Awf1x"}},{"cell_type":"code","source":["!git lfs install"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fPHjeFDkvj-F","outputId":"6faf0565-1a01-4212-8a96-6595abfd79ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Git LFS initialized.\n"]}]},{"cell_type":"code","source":["!git clone https://huggingface.co/datasets/IlyaGusev/ru_turbo_saiga"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mnEX8OLbvlbh","outputId":"38d05acf-3de3-4f47-ea2a-6f4537f95ecd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'ru_turbo_saiga'...\n","remote: Enumerating objects: 58, done.\u001b[K\n","remote: Total 58 (delta 0), reused 0 (delta 0), pack-reused 58 (from 1)\u001b[K\n","Unpacking objects: 100% (58/58), 7.92 KiB | 426.00 KiB/s, done.\n"]}]},{"cell_type":"markdown","source":["Файл сжат алгоритмом Zstandard, поэтому декомпрессируем его"],"metadata":{"id":"Dsz-zO0xwj1y"}},{"cell_type":"code","source":["!sudo apt install zstd -y"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c3rPvGVDwQ03","outputId":"afe39e48-eb6f-41b8-e826-aad9946d2676"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following NEW packages will be installed:\n","  zstd\n","0 upgraded, 1 newly installed, 0 to remove and 29 not upgraded.\n","Need to get 603 kB of archives.\n","After this operation, 1,695 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 zstd amd64 1.4.8+dfsg-3build1 [603 kB]\n","Fetched 603 kB in 1s (812 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package zstd.\n","(Reading database ... 126209 files and directories currently installed.)\n","Preparing to unpack .../zstd_1.4.8+dfsg-3build1_amd64.deb ...\n","Unpacking zstd (1.4.8+dfsg-3build1) ...\n","Setting up zstd (1.4.8+dfsg-3build1) ...\n","Processing triggers for man-db (2.10.2-1) ...\n"]}]},{"cell_type":"code","source":["!zstd -d /content/ru_turbo_saiga/ru_turbo_saiga.jsonl.zst"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rqOAePLCwTCc","outputId":"7a9d1229-4dfb-4ca5-dde1-a6376615f99e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/ru_turbo_saiga/ru_turbo_saiga.jsonl.zst: 93186853 bytes \n"]}]},{"cell_type":"code","source":["import torch\n","from peft import AutoPeftModelForCausalLM\n","from transformers import AutoTokenizer, pipeline\n","from datasets import load_dataset\n","from random import randint\n","\n","# Adapter ID\n","peft_model_id = \"IlyaGusev/saiga_mistral_7b_lora\"\n","\n","# Base model\n","base_model_name = \"Open-Orca/Mistral-7B-OpenOrca\"\n","\n","# Load model with PEFT adapter\n","model = AutoPeftModelForCausalLM.from_pretrained(\n"," peft_model_id, # adapter\n"," device_map={\"\": 0}, # automatically detect device\n"," torch_dtype=torch.float16\n",")\n","tokenizer = AutoTokenizer.from_pretrained(\n"," base_model_name, # base model\n"," trust_remote_code=True\n",")\n","\n","tokenizer.pad_token = tokenizer.eos_token # define token separator\n","\n","# Load into pipeline\n","pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n","\n","# Load test dataset\n","eval_dataset = load_dataset(\"json\", data_files=\"ru_turbo_saiga/ru_turbo_saiga.jsonl\", split=\"train\")\n","\n","# Select a random index\n","rand_idx = randint(0, len(eval_dataset) - 1)\n","\n","# Test pattern\n","prompt = tokenizer.apply_chat_template(eval_dataset[rand_idx][\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n","outputs = pipe(prompt, max_new_tokens=120,\n","               temperature=0.2, top_p=0.95, do_sample=True,\n","               eos_token_id=tokenizer.eos_token_id,\n","               pad_token_id=tokenizer.pad_token_id)\n","\n","print()\n","print(\"Query:\")\n","print()\n","print(eval_dataset[rand_idx]['messages'][1]['content'])\n","print()\n","print(\"Original answer:\")\n","print()\n","print(eval_dataset[rand_idx]['messages'][2]['content'])\n","print()\n","print(\"Generated model response:\")\n","print()\n","print(outputs[0]['generated_text'][len(prompt):].strip())\n","print()"],"metadata":{"id":"GSTgO8pjPAHc"},"execution_count":null,"outputs":[]}]}